{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11337930,"sourceType":"datasetVersion","datasetId":7092765}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install vncorenlp\n!pip install transformers torch\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:02:34.812139Z","iopub.execute_input":"2025-04-09T10:02:34.812336Z","iopub.status.idle":"2025-04-09T10:02:45.361822Z","shell.execute_reply.started":"2025-04-09T10:02:34.812317Z","shell.execute_reply":"2025-04-09T10:02:45.360887Z"}},"outputs":[{"name":"stdout","text":"Collecting vncorenlp\n  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2025.1.31)\nBuilding wheels for collected packages: vncorenlp\n  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645932 sha256=e0a0c462903ca22b40ac57788dfa2e0f1ec07ccc491e8e3455a6a43dc0712f73\n  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\nSuccessfully built vncorenlp\nInstalling collected packages: vncorenlp\nSuccessfully installed vncorenlp-1.0.3\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:02:45.362986Z","iopub.execute_input":"2025-04-09T10:02:45.363244Z","iopub.status.idle":"2025-04-09T10:02:46.405228Z","shell.execute_reply.started":"2025-04-09T10:02:45.363222Z","shell.execute_reply":"2025-04-09T10:02:46.404596Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\n# Tải tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\n# Tải mô hình pretrained PhoBERT (encoder)\nmodel = AutoModel.from_pretrained(\"vinai/phobert-base\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:02:46.407136Z","iopub.execute_input":"2025-04-09T10:02:46.407482Z","iopub.status.idle":"2025-04-09T10:03:12.872994Z","shell.execute_reply.started":"2025-04-09T10:02:46.407460Z","shell.execute_reply":"2025-04-09T10:03:12.871702Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad58e2973b504098979e608bf12eb105"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5bf0c1c0aed4003a635eaf693d052d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b083ae9fdd341ae8a456884518083f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6243abdf348b4b259cd1516cb3c31f6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9cd73e93987433fb31a55a113549c81"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"**READ DATA*> ***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_excel(\"/kaggle/input/vnexpress/TopicModeling_Final.xlsx\", engine='openpyxl')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:12.875551Z","iopub.execute_input":"2025-04-09T10:03:12.877221Z","iopub.status.idle":"2025-04-09T10:03:14.140320Z","shell.execute_reply.started":"2025-04-09T10:03:12.877192Z","shell.execute_reply":"2025-04-09T10:03:14.139474Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                    Category  \\\n0         Văn hóa & lối sống   \n1      Kinh doanh & quản trị   \n2            Y tế & sức khỏe   \n3     Chính trị & chính sách   \n4         Văn hóa & lối sống   \n...                      ...   \n3328              Môi trường   \n3329   Kinh doanh & quản trị   \n3330   Kinh doanh & quản trị   \n3331     Giáo dục & tri thức   \n3332      Văn hóa & lối sống   \n\n                                                Content  \n0     Bái Đính cổ tự là ngôi chùa được xây dựng trên...  \n1     Vận động viên của Singapore có thể là luật sư,...  \n2     Đầu năm học mới, tôi cân thử chiếc ba lô của c...  \n3     “Tâm tư của tôi cái gì luật không cấm thì phải...  \n4     Mẹ về nhà, lưng áo đẫm mồ hôi, ngồi đếm những ...  \n...                                                 ...  \n3328  Chúng ta đang rửa tay theo cách rất khác với m...  \n3329  Tôi gặp Chokchai Koisrichai - giám đốc, nhà đi...  \n3330  Suốt bao năm tôi ở nhà thuê vì không biết sau ...  \n3331  Tôi vừa đi khảo sát một số trường học ở một hu...  \n3332  Khi chúng tôi quyên góp giúp bà con Sài Gòn đa...  \n\n[3333 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Văn hóa &amp; lối sống</td>\n      <td>Bái Đính cổ tự là ngôi chùa được xây dựng trên...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Kinh doanh &amp; quản trị</td>\n      <td>Vận động viên của Singapore có thể là luật sư,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Y tế &amp; sức khỏe</td>\n      <td>Đầu năm học mới, tôi cân thử chiếc ba lô của c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Chính trị &amp; chính sách</td>\n      <td>“Tâm tư của tôi cái gì luật không cấm thì phải...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Văn hóa &amp; lối sống</td>\n      <td>Mẹ về nhà, lưng áo đẫm mồ hôi, ngồi đếm những ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3328</th>\n      <td>Môi trường</td>\n      <td>Chúng ta đang rửa tay theo cách rất khác với m...</td>\n    </tr>\n    <tr>\n      <th>3329</th>\n      <td>Kinh doanh &amp; quản trị</td>\n      <td>Tôi gặp Chokchai Koisrichai - giám đốc, nhà đi...</td>\n    </tr>\n    <tr>\n      <th>3330</th>\n      <td>Kinh doanh &amp; quản trị</td>\n      <td>Suốt bao năm tôi ở nhà thuê vì không biết sau ...</td>\n    </tr>\n    <tr>\n      <th>3331</th>\n      <td>Giáo dục &amp; tri thức</td>\n      <td>Tôi vừa đi khảo sát một số trường học ở một hu...</td>\n    </tr>\n    <tr>\n      <th>3332</th>\n      <td>Văn hóa &amp; lối sống</td>\n      <td>Khi chúng tôi quyên góp giúp bà con Sài Gòn đa...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3333 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Bước 1: Chuyển chữ thường, strip và chuẩn hóa unicode\nimport unicodedata\n\ndef clean_label(label):\n    label = unicodedata.normalize('NFC', label)\n    label = label.lower().strip()\n    label = label.replace(\"  \", \" \")  # nếu có double space\n    return label\n\ndf['Category_clean'] = df['Category'].apply(clean_label)\n\n# Bước 2: Mapping về nhãn chuẩn (tên đẹp)\ncategory_mapping = {\n    'văn hóa & lối sống': 'Văn hóa & lối sống',\n    'chính trị & chính sách': 'Chính trị & chính sách',\n    'kinh doanh & quản trị': 'Kinh doanh & quản trị',\n    'giáo dục & tri thức': 'Giáo dục & tri thức',\n    'y tế & sức khỏe': 'Y tế & sức khỏe',\n    'môi trường': 'Môi trường'\n    }\n\n# Bước 3: Gán lại nhãn cuối\ndf['Category_standard'] = df['Category_clean'].map(category_mapping)\ndf['Category_standard'].value_counts()\n\ndf = df.drop(columns=['Category_clean', 'Category'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:14.141190Z","iopub.execute_input":"2025-04-09T10:03:14.141776Z","iopub.status.idle":"2025-04-09T10:03:14.169354Z","shell.execute_reply.started":"2025-04-09T10:03:14.141751Z","shell.execute_reply":"2025-04-09T10:03:14.168596Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                Content  \\\n0     Bái Đính cổ tự là ngôi chùa được xây dựng trên...   \n1     Vận động viên của Singapore có thể là luật sư,...   \n2     Đầu năm học mới, tôi cân thử chiếc ba lô của c...   \n3     “Tâm tư của tôi cái gì luật không cấm thì phải...   \n4     Mẹ về nhà, lưng áo đẫm mồ hôi, ngồi đếm những ...   \n...                                                 ...   \n3328  Chúng ta đang rửa tay theo cách rất khác với m...   \n3329  Tôi gặp Chokchai Koisrichai - giám đốc, nhà đi...   \n3330  Suốt bao năm tôi ở nhà thuê vì không biết sau ...   \n3331  Tôi vừa đi khảo sát một số trường học ở một hu...   \n3332  Khi chúng tôi quyên góp giúp bà con Sài Gòn đa...   \n\n           Category_standard  \n0         Văn hóa & lối sống  \n1      Kinh doanh & quản trị  \n2            Y tế & sức khỏe  \n3     Chính trị & chính sách  \n4         Văn hóa & lối sống  \n...                      ...  \n3328              Môi trường  \n3329   Kinh doanh & quản trị  \n3330   Kinh doanh & quản trị  \n3331     Giáo dục & tri thức  \n3332      Văn hóa & lối sống  \n\n[3333 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Content</th>\n      <th>Category_standard</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bái Đính cổ tự là ngôi chùa được xây dựng trên...</td>\n      <td>Văn hóa &amp; lối sống</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Vận động viên của Singapore có thể là luật sư,...</td>\n      <td>Kinh doanh &amp; quản trị</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Đầu năm học mới, tôi cân thử chiếc ba lô của c...</td>\n      <td>Y tế &amp; sức khỏe</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>“Tâm tư của tôi cái gì luật không cấm thì phải...</td>\n      <td>Chính trị &amp; chính sách</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Mẹ về nhà, lưng áo đẫm mồ hôi, ngồi đếm những ...</td>\n      <td>Văn hóa &amp; lối sống</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3328</th>\n      <td>Chúng ta đang rửa tay theo cách rất khác với m...</td>\n      <td>Môi trường</td>\n    </tr>\n    <tr>\n      <th>3329</th>\n      <td>Tôi gặp Chokchai Koisrichai - giám đốc, nhà đi...</td>\n      <td>Kinh doanh &amp; quản trị</td>\n    </tr>\n    <tr>\n      <th>3330</th>\n      <td>Suốt bao năm tôi ở nhà thuê vì không biết sau ...</td>\n      <td>Kinh doanh &amp; quản trị</td>\n    </tr>\n    <tr>\n      <th>3331</th>\n      <td>Tôi vừa đi khảo sát một số trường học ở một hu...</td>\n      <td>Giáo dục &amp; tri thức</td>\n    </tr>\n    <tr>\n      <th>3332</th>\n      <td>Khi chúng tôi quyên góp giúp bà con Sài Gòn đa...</td>\n      <td>Văn hóa &amp; lối sống</td>\n    </tr>\n  </tbody>\n</table>\n<p>3333 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from sklearn.utils import resample\n\nmax_count = df['Category_standard'].value_counts().max()\nbalanced_dfs = []\n\nfor label in df['Category_standard'].unique():\n    group = df[df['Category_standard'] == label]\n    resampled = resample(group, replace=True, n_samples=max_count, random_state=42)\n    balanced_dfs.append(resampled)\n\ndf = pd.concat(balanced_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:14.170190Z","iopub.execute_input":"2025-04-09T10:03:14.170465Z","iopub.status.idle":"2025-04-09T10:03:14.193617Z","shell.execute_reply.started":"2025-04-09T10:03:14.170442Z","shell.execute_reply":"2025-04-09T10:03:14.192969Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import unicodedata\nimport re\nimport string\nimport pandas as pd\nfrom transformers import AutoTokenizer\n\n# Load PhoBERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\n# Load stopwords từ file \"vietnamese-stopwords.txt\"\ndef load_stopwords(filepath):\n    with open(\"/kaggle/input/vnexpress/vietnamese-stopwords.txt\", encoding='utf-8') as f:\n        stopwords = set(line.strip() for line in f if line.strip())\n\n    return stopwords\n\nstopwords = load_stopwords(\"vietnamese-stopwords.txt\")\n\n# 1. Chuẩn hóa unicode NFC\ndef normalize_unicode(text):\n    return unicodedata.normalize('NFC', text)\n\n# 2. Làm sạch văn bản: viết thường, bỏ số, dấu câu, khoảng trắng thừa\ndef text_normalizer(text):\n    text = text.lower()\n    text = re.sub(r'\\d+', '', text)  \n    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n    text = re.sub(\"\\s+\", \" \", text).strip()\n    return text\n\n# 3. Loại bỏ stopwords\ndef remove_stopwords(text, stopwords):\n    tokens = text.split()\n    filtered = [word for word in tokens if word not in stopwords]\n    return \" \".join(filtered)\n\n# 4. Tiền xử lý văn bản thành văn bản sạch (không mã hóa)\ndef clean_text_for_phobert(text, stopwords):\n    text = normalize_unicode(text)\n    text = text_normalizer(text)\n    text = remove_stopwords(text, stopwords)\n    return text\n\n# 5. Tokenize thành input_ids\ndef encode_text(text, max_len=256):\n    return tokenizer.encode(\n        text,\n        max_length=max_len,\n        truncation=True,\n        padding='max_length'\n    )\n\n# --- Áp dụng lên DataFrame ---\n# df là DataFrame đã có cột 'Content'\n\n# Tạo cột văn bản đã xử lý\ndf['Content_cleaned'] = df['Content'].apply(lambda x: clean_text_for_phobert(x, stopwords))\n\n# Tạo cột input_ids cho PhoBERT\ndf['input_ids'] = df['Content_cleaned'].apply(lambda x: encode_text(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:14.194319Z","iopub.execute_input":"2025-04-09T10:03:14.194503Z","iopub.status.idle":"2025-04-09T10:03:33.549119Z","shell.execute_reply.started":"2025-04-09T10:03:14.194487Z","shell.execute_reply":"2025-04-09T10:03:33.548482Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category_standard'])  \nnum_labels = len(le.classes_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:33.549946Z","iopub.execute_input":"2025-04-09T10:03:33.550237Z","iopub.status.idle":"2025-04-09T10:03:33.556319Z","shell.execute_reply.started":"2025-04-09T10:03:33.550207Z","shell.execute_reply":"2025-04-09T10:03:33.555462Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df_final = df.drop(columns = ['Content', 'Category_standard', 'Content_cleaned'])\ndf_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:33.558670Z","iopub.execute_input":"2025-04-09T10:03:33.558981Z","iopub.status.idle":"2025-04-09T10:03:33.767007Z","shell.execute_reply.started":"2025-04-09T10:03:33.558953Z","shell.execute_reply":"2025-04-09T10:03:33.766147Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                              input_ids  label\n0     [0, 441, 9866, 1560, 1294, 4721, 1103, 2935, 2...      2\n1     [0, 2404, 6928, 2487, 229, 1824, 3078, 7564, 6...      2\n2     [0, 328, 2201, 328, 9667, 2710, 1713, 12053, 8...      2\n3     [0, 61610, 10893, 49592, 1701, 564, 61610, 353...      0\n4     [0, 13397, 409, 109, 441, 1171, 2497, 2194, 61...      2\n...                                                 ...    ...\n4759  [0, 4368, 1430, 1895, 14294, 418, 119, 289, 74...      1\n4760  [0, 238, 222, 4698, 1746, 401, 940, 4698, 1031...      4\n4761  [0, 2925, 2792, 1197, 9645, 1080, 30768, 2183,...      0\n4762  [0, 286, 9393, 2288, 18116, 176, 853, 2615, 85...      3\n4763  [0, 235, 5963, 2546, 681, 808, 2546, 5717, 307...      2\n\n[4764 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0, 441, 9866, 1560, 1294, 4721, 1103, 2935, 2...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0, 2404, 6928, 2487, 229, 1824, 3078, 7564, 6...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0, 328, 2201, 328, 9667, 2710, 1713, 12053, 8...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0, 61610, 10893, 49592, 1701, 564, 61610, 353...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0, 13397, 409, 109, 441, 1171, 2497, 2194, 61...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4759</th>\n      <td>[0, 4368, 1430, 1895, 14294, 418, 119, 289, 74...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4760</th>\n      <td>[0, 238, 222, 4698, 1746, 401, 940, 4698, 1031...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4761</th>\n      <td>[0, 2925, 2792, 1197, 9645, 1080, 30768, 2183,...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4762</th>\n      <td>[0, 286, 9393, 2288, 18116, 176, 853, 2615, 85...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4763</th>\n      <td>[0, 235, 5963, 2546, 681, 808, 2546, 5717, 307...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>4764 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# === Chia tập dữ liệu train - val - test\ntrain_val_df, test_df = train_test_split(df_final, test_size=0.15, random_state=42, stratify=df_final['label'])\ntrain_df, val_df = train_test_split(train_val_df, test_size=0.15, random_state=42, stratify=train_val_df['label'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:33.768071Z","iopub.execute_input":"2025-04-09T10:03:33.768294Z","iopub.status.idle":"2025-04-09T10:03:33.803561Z","shell.execute_reply.started":"2025-04-09T10:03:33.768274Z","shell.execute_reply":"2025-04-09T10:03:33.802674Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass PhoBertDataset(Dataset):\n    def __init__(self, data):\n        self.input_ids = data['input_ids'].tolist()\n        self.labels = data['label'].tolist()\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n            'attention_mask': torch.tensor([1 if id != 1 else 0 for id in self.input_ids[idx]], dtype=torch.long),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n    def __len__(self):\n        return len(self.labels)\n\n\n# === Khởi tạo datasets\ntrain_dataset = PhoBertDataset(train_df)\nval_dataset = PhoBertDataset(val_df)\ntest_dataset = PhoBertDataset(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:33.804493Z","iopub.execute_input":"2025-04-09T10:03:33.804867Z","iopub.status.idle":"2025-04-09T10:03:33.811354Z","shell.execute_reply.started":"2025-04-09T10:03:33.804836Z","shell.execute_reply":"2025-04-09T10:03:33.810506Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n# Không cần import compute_class_weight nếu không dùng class weights\n# from sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n# === Cấu hình số lượng nhãn\nnum_labels = df_final['label'].nunique()\n\n# === Tải mô hình PhoBERT\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"vinai/phobert-base\",\n    num_labels=num_labels\n)\n\n# === Cấu hình training\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=6,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    greater_is_better=True,\n    logging_dir=\"./logs\",\n    report_to=\"none\"\n)\n\n# === Hàm tính metric\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"precision_macro\": precision_score(labels, preds, average='macro'),\n        \"recall_macro\": recall_score(labels, preds, average='macro'),\n        \"f1_macro\": f1_score(labels, preds, average='macro'),\n    }\n\n# === Dùng Trainer mặc định (không class weights)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# === Huấn luyện mô hình\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:05:25.507788Z","iopub.execute_input":"2025-04-09T10:05:25.508125Z","iopub.status.idle":"2025-04-09T10:17:04.513686Z","shell.execute_reply.started":"2025-04-09T10:05:25.508101Z","shell.execute_reply":"2025-04-09T10:17:04.512908Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1296' max='1296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1296/1296 11:36, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision Macro</th>\n      <th>Recall Macro</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.199600</td>\n      <td>0.902102</td>\n      <td>0.712171</td>\n      <td>0.711742</td>\n      <td>0.712693</td>\n      <td>0.709739</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.732900</td>\n      <td>0.745101</td>\n      <td>0.761513</td>\n      <td>0.762578</td>\n      <td>0.762053</td>\n      <td>0.757034</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.548200</td>\n      <td>0.654509</td>\n      <td>0.805921</td>\n      <td>0.807784</td>\n      <td>0.806203</td>\n      <td>0.806415</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.425000</td>\n      <td>0.631246</td>\n      <td>0.820724</td>\n      <td>0.819954</td>\n      <td>0.821167</td>\n      <td>0.817916</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.326000</td>\n      <td>0.639385</td>\n      <td>0.830592</td>\n      <td>0.829747</td>\n      <td>0.830923</td>\n      <td>0.828701</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.275700</td>\n      <td>0.624480</td>\n      <td>0.847039</td>\n      <td>0.845964</td>\n      <td>0.847344</td>\n      <td>0.845811</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1296, training_loss=0.5845523675282797, metrics={'train_runtime': 697.3696, 'train_samples_per_second': 29.606, 'train_steps_per_second': 1.858, 'total_flos': 2716192971380736.0, 'train_loss': 0.5845523675282797, 'epoch': 6.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# === Đánh giá mô hình trên tập test\ntest_metrics = trainer.evaluate(test_dataset)\nprint(\"Test set evaluation:\")\nfor k, v in test_metrics.items():\n    print(f\"{k}: {v:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:19:27.233481Z","iopub.execute_input":"2025-04-09T10:19:27.233872Z","iopub.status.idle":"2025-04-09T10:19:34.254435Z","shell.execute_reply.started":"2025-04-09T10:19:27.233845Z","shell.execute_reply":"2025-04-09T10:19:34.253733Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Test set evaluation:\neval_loss: 0.5624\neval_accuracy: 0.8573\neval_precision_macro: 0.8580\neval_recall_macro: 0.8572\neval_f1_macro: 0.8569\neval_runtime: 7.0133\neval_samples_per_second: 101.9490\neval_steps_per_second: 6.4160\nepoch: 6.0000\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"preds_output = trainer.predict(test_dataset)\npred_labels = np.argmax(preds_output.predictions, axis=1)\n\n# Nếu muốn lưu để phân tích sau:\ntest_df[\"pred_label\"] = pred_labels\ntest_df.to_excel(\"test_predictions.xlsx\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:19:50.230254Z","iopub.execute_input":"2025-04-09T10:19:50.230590Z","iopub.status.idle":"2025-04-09T10:19:57.492266Z","shell.execute_reply.started":"2025-04-09T10:19:50.230559Z","shell.execute_reply":"2025-04-09T10:19:57.491273Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(test_df['label'], pred_labels)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(xticks_rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T10:03:40.781797Z","iopub.status.idle":"2025-04-09T10:03:40.782084Z","shell.execute_reply":"2025-04-09T10:03:40.781957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}